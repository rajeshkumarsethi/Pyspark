# -*- coding: utf-8 -*-
"""Spark_Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZdAa2_Bg0ZX01ywLu3mstNApGfdC4cd_
"""

!pip install pyspark

# import SparkSession
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()

# Create RDD from parallelize 
# Converting a list into RDD (Resilient Distributed DataList)
dataList = [("Java", 20000), ("Python",100000), ("Scala",3000)]
rdd = spark.sparkContext.parallelize(dataList)

# Count of RDD
rdd.count()

# retrieve all the elements of the dataset 
rdd.collect()

# Return the first element in the dataset
rdd.first()

# Return the maximum value from the dataset.
rdd.max()
# Basically it takes the maximum key

# Return the minimum value from the dataset.
rdd.min()

# Create DataFrame Using createDataFrame()
data = [('James','','Smith','1991-04-01','M',3000),
  ('Michael','Rose','','2000-05-19','M',4000),
  ('Robert','','Williams','1978-09-05','M',4000),
  ('Maria','Anne','Jones','1967-12-01','F',4000),
  ('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname","middlename","lastname","dob","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)

# shows the 20 elements from the DataFrame
df.show()

# Displays the Layout of the dataframe. It includes column names and the data types of each columns
df.printSchema()

"""## Create DataFrame from RDD"""

columns = ["language","users_count"]
data = [("Java", "20000"), ("Python", "100000"), ("Scala", "3000")]

spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
rdd = spark.sparkContext.parallelize(data)

# Using toDF() function
dfFromRDD1 = rdd.toDF()

dfFromRDD1.printSchema()

# Adding columns to the dataframe
dfFromRDD1 = rdd.toDF(columns)

dfFromRDD1.printSchema()

## Using createDataFrame() from SparkSession
dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)

dfFromRDD2.printSchema()

"""## Create DataFrame from List Collection"""

# Using createDataFrame() from SparkSession
dfFromData2 = spark.createDataFrame(data).toDF(*columns)

dfFromData2.printSchema()

"""## Create DataFrame with schema"""

from pyspark.sql.types import StructType,StructField, StringType, IntegerType
data2 = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
  ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
  ])
 
df = spark.createDataFrame(data=data2,schema=schema)
df.printSchema()
df.show(truncate=False)

"""# Convert PySpark DataFrame to Pandas"""

# Convert PySpark Dataframe to Pandas
pandasDF = df.toPandas()
print(pandasDF)

pandasDF.shape

# Default - displays 20 rows and 
# 20 charactes from column value 
df.show()

#Display full column contents
df.show(truncate = False)

# Display 2 rows and full column contents
df.show(2, truncate = False)

# Display 2 rows & column values 5 characters
df.show(2,truncate=5)

# Display DataFrame rows & columns vertically
df.show(n=3,truncate=25,vertical=True)

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
columns = ["Seqno","Quote"]
data = [("1", "Be the change that you wish to see in the world"),
    ("2", "Everyone thinks of changing the world, but no one thinks of changing himself."),
    ("3", "The purpose of our lives is to be happy."),
    ("4", "Be cool.")]
df = spark.createDataFrame(data,columns)
df.show()

df.show(truncate=10)

"""## Convert Spark Nested Struct DataFrame to Pandas"""

# Nested structure elements
from pyspark.sql.types import StructType, StructField, StringType,IntegerType
structureData = [
    (("James","","Smith"),"36636","M",3100),
    (("Michael","Rose",""),"40288","M",4300),
    (("Robert","","Williams"),"42114","M",1400),
    (("Maria","Anne","Jones"),"39192","F",5500),
    (("Jen","Mary","Brown"),"","F",-1)
  ]
structureSchema = StructType([
        StructField('name', StructType([
             StructField('firstname', StringType(), True),
             StructField('middlename', StringType(), True),
             StructField('lastname', StringType(), True)
             ])),
         StructField('id', StringType(), True),
         StructField('gender', StringType(), True),
         StructField('salary', IntegerType(), True)
         ])

df2 = spark.createDataFrame(data=structureData,schema=structureSchema)
df2.printSchema()
df2.show(truncate=False)

df2.show(truncate=False)

"""# Adding & Changing struct of the DataFrame"""

from pyspark.sql.functions import col,struct,when
updatedDF = df2.withColumn("OtherInfo", 
    struct(col("id").alias("identifier"),
    col("gender").alias("gender"),
    col("salary").alias("salary"),
    when(col("salary").cast(IntegerType()) < 2000,"Low")
      .when(col("salary").cast(IntegerType()) < 4000,"Medium")
      .otherwise("High").alias("Salary_Grade")
  )).drop("id","gender","salary")

updatedDF.printSchema()
updatedDF.show(truncate=False)

